{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabula as tb\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly\n",
    "import math\n",
    "import networkx as nx\n",
    "from matplotlib import cm\n",
    "import numpy as np\n",
    "import matplotlib.gridspec as gridspec\n",
    "import mplcursors\n",
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "\n",
    "class NamedDataFrame(pd.DataFrame):\n",
    "    \"\"\"\n",
    "    A custom DataFrame with an associated name attribute.\n",
    "\n",
    "    Parameters:\n",
    "        *args, **kwargs: Passed to the pandas DataFrame constructor.\n",
    "\n",
    "    Attributes:\n",
    "        name (str): A name associated with the DataFrame.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize a NamedDataFrame.\n",
    "\n",
    "        Parameters:\n",
    "            *args, **kwargs: Passed to the pandas DataFrame constructor.\n",
    "        \"\"\"\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.name = None\n",
    "\n",
    "\n",
    "\n",
    "def csv_to_unique_vals_dict(csv_paths, columns_list):\n",
    "    df_list = []\n",
    "    unique_values_dict = {}\n",
    "\n",
    "    for csv_path in csv_paths:\n",
    "        df_name = f'{os.path.splitext(os.path.basename(csv_path))[0]}'\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df_list.append(df)\n",
    "\n",
    "        # Get unique values for the specified columns and present it as a {'column_key': num_of_unique_val}\n",
    "        unique_values = df[columns_list].apply(lambda x: len(x.unique())).to_dict()\n",
    "        unique_values_dict[df_name] = unique_values\n",
    "\n",
    "    return df_list, unique_values_dict\n",
    "\n",
    "\n",
    "\n",
    "def csv_to_common_vals_dict(df_list, columns_list):\n",
    "    common_unique_values = {}\n",
    "\n",
    "    for df in df_list:\n",
    "        for column in df[columns_list].columns:\n",
    "            unique_values_column = set(df[column].unique())\n",
    "\n",
    "            if column in common_unique_values:\n",
    "                common_unique_values[column].intersection_update(unique_values_column)\n",
    "            else:\n",
    "                common_unique_values[column] = unique_values_column.copy()\n",
    "\n",
    "    return common_unique_values\n",
    "\n",
    "\n",
    "\n",
    "def plot_common_vals(common_unique_values):\n",
    "    # Plotting a bar chart with specified colors\n",
    "    fig, ax = plt.subplots()\n",
    "    colors = sns.dark_palette(\"navy\", n_colors=len(common_unique_values))\n",
    "\n",
    "    # Iterate over the common_unique_values dictionary\n",
    "    for i, (column_name, values_list) in enumerate(common_unique_values.items()):\n",
    "        bar_height = len(values_list)\n",
    "\n",
    "        # Plot each bar\n",
    "        ax.bar(i, bar_height, color=colors[i], label=column_name)\n",
    "\n",
    "        # Annotate each bar with the exact number and list of elements (inside the bar)\n",
    "        annotation_text = f'{bar_height}\\n'\n",
    "        annotation_text += '\\n'.join(values_list)\n",
    "\n",
    "        ax.text(i, bar_height / 2, annotation_text,\n",
    "                ha='center', va='center', color='white')\n",
    "\n",
    "    # Set x-axis labels and title\n",
    "    ax.set_xticks(range(len(common_unique_values)))\n",
    "    ax.set_xticklabels(common_unique_values.keys())\n",
    "    #plt.title('Comparison of Common Unique Values')\n",
    "    #plt.xlabel('Columns')\n",
    "    plt.ylabel('Number of Common Unique Values')\n",
    "\n",
    "    plt.savefig('../results/step1_pdf_parsing/plots/common_vals.svg')\n",
    "\n",
    "\n",
    "\n",
    "def plot_unique_vals(unique_values_dict, columns_list):\n",
    "    # Convert the dictionary to a DataFrame for plotting\n",
    "    unique_values_df = pd.DataFrame(unique_values_dict)\n",
    "\n",
    "    # Plotting a bar chart with specified colors\n",
    "    colors = sns.dark_palette(\"navy\", n_colors=len(columns_list))\n",
    "    ax = unique_values_df.plot(kind='bar', rot=0, color=colors)\n",
    "    #plt.title('Comparison of Unique Values')\n",
    "    plt.ylabel('Number of Unique Values')\n",
    "\n",
    "    # Annotate each bar with a number of elements\n",
    "    for p in ax.patches:\n",
    "        ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                    ha='center', va='center', xytext=(0, 5), textcoords='offset points', clip_on=False)\n",
    "\n",
    "    plt.savefig('../results/step1_pdf_parsing/plots/unique_vals_with_comparison.svg')\n",
    "\n",
    "\n",
    "\n",
    "def vz_comparison_of_csvs(csv_paths, columns_list):\n",
    "\n",
    "    df_list, unique_values_dict = csv_to_unique_vals_dict(csv_paths, columns_list)\n",
    "    common_unique_values = csv_to_common_vals_dict(df_list, columns_list)\n",
    "\n",
    "    plot_common_vals(common_unique_values)\n",
    "    plot_unique_vals(unique_values_dict, columns_list)\n",
    "\n",
    "\n",
    "\n",
    "def rename_dataframe_columns(dataframe, original_column_names, new_column_names):\n",
    "    \"\"\"\n",
    "    Rename columns in a DataFrame based on specified old and new column names.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The DataFrame to be modified.\n",
    "        old_column_names (list): A list of old column names to be replaced.\n",
    "        new_column_names (list): A list of new column names to replace the old ones.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with updated column names.\n",
    "    \"\"\"\n",
    "    header_names_list = ['Chemical', 'PDB', 'Protein']\n",
    "\n",
    "    # Check if at least one element of old_column_names is present in header_names_list\n",
    "    contains_common_element = any(elem in original_column_names for elem in header_names_list)\n",
    "\n",
    "    if contains_common_element:\n",
    "        return dataframe.drop(index=0).rename(columns=dict(zip(original_column_names, new_column_names))).reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        # Rename columns using new_row_dict\n",
    "        dataframe = dataframe.rename(columns=dict(zip(original_column_names, new_column_names)))\n",
    "\n",
    "        # Create a new row with data from renamed row\n",
    "        new_row = dict(zip(new_column_names, original_column_names))\n",
    "\n",
    "        # Add a new row at the first place\n",
    "        return pd.concat([pd.DataFrame(new_row, index=[0]), dataframe], ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "def read_pdf_and_create_dataframe(page_from, page_till, pdf_path):\n",
    "    \"\"\"\n",
    "    Read PDF pages and create a DataFrame by concatenating data from each page.\n",
    "\n",
    "    Parameters:\n",
    "        page_from (int): The starting page number.\n",
    "        page_till (int): The ending page number + 1 (exclusive).\n",
    "        pdf_path (str): The path to the PDF file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame or None: The concatenated DataFrame or None if no data is found.\n",
    "    \"\"\"\n",
    "    all_dataframes = []\n",
    "\n",
    "    for page_number in tqdm(range(page_from, page_till), desc='Page parsing', total=page_till-page_from):\n",
    "        try:\n",
    "            dataframe_list = tb.read_pdf(pdf_path, pages=str(page_number))\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading PDF page {page_number}: {e}\")\n",
    "            continue\n",
    "\n",
    "        if dataframe_list and len(dataframe_list[0].columns) in [4, 5]:\n",
    "            original_column_names = dataframe_list[0].columns\n",
    "            common_new_names = ['Chemical_ID', 'PDB_ID', 'Protein_Name']\n",
    "\n",
    "            if len(dataframe_list[0].columns) == 4:\n",
    "                new_column_names = common_new_names + ['Torsion_Angle']\n",
    "            elif len(dataframe_list[0].columns) == 5:\n",
    "                new_column_names = common_new_names + ['Torsion_Angle_1', 'Torsion_Angle_2']\n",
    "            else:\n",
    "                print('Table does not match the expected format')\n",
    "                continue\n",
    "\n",
    "            renamed_dataframe = rename_dataframe_columns(dataframe_list[0], original_column_names, new_column_names)\n",
    "            all_dataframes.append(renamed_dataframe)\n",
    "        else:\n",
    "            print(f'No data found in PDF page {page_number}.')\n",
    "\n",
    "    if all_dataframes:\n",
    "\n",
    "        # Concatenate all DataFrames vertically\n",
    "        return pd.concat(all_dataframes, axis=0).reset_index(drop=True)\n",
    "\n",
    "    else:\n",
    "        print('No data found in the specified pages.')\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def parse_pdf(pdf_path):\n",
    "\n",
    "    # Setting page numbers of the source PDF for each DataFrame to be formed: [start_page, end_page+1]\n",
    "    df_shapes = {'pyridones_scaffold': [3, 6], 'diarylamines_scaffold': [6, 32]}\n",
    "    df_names = list(df_shapes.keys())\n",
    "    named_df_list = []\n",
    "\n",
    "    # Creating named DataFrames of set shapes\n",
    "    for key, name in tqdm(zip(df_shapes.keys(), df_names), desc=\"Processing PDFs\", total=len(df_shapes)):\n",
    "        df = read_pdf_and_create_dataframe(df_shapes[key][0], df_shapes[key][1], pdf_path)\n",
    "        if df is not None and not df.empty:\n",
    "            named_df = NamedDataFrame(df)\n",
    "            named_df.name = name\n",
    "            named_df_list.append(named_df)\n",
    "        else:\n",
    "            print(f'Error during reading the .pdf file into a DataFrame: {name}')\n",
    "\n",
    "    return named_df_list\n",
    "\n",
    "\n",
    "\n",
    "def save_dataframe_to_csv(dataframe, file_name, directory='../results/step1_pdf_parsing/no_mapping_csvs'):\n",
    "    \"\"\"\n",
    "    Save a DataFrame to a CSV file.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pd.DataFrame): The DataFrame to be saved.\n",
    "        file_name (str): The name of the CSV file (without extension).\n",
    "        directory (str): The directory where the CSV file will be saved.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the operation is successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ensure the directory exists, create it if not\n",
    "        os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "        # Construct the full path for the CSV file\n",
    "        csv_file_path = os.path.join(directory, f'{file_name}.csv')\n",
    "\n",
    "        # Write the DataFrame to the CSV file\n",
    "        dataframe.to_csv(csv_file_path, index=False)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'Error saving DataFrame to a CSV: {e}')\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def parsing(parsing_flag, pdf_path):\n",
    "\n",
    "    if parsing_flag:\n",
    "        named_df_list = parse_pdf(pdf_path)\n",
    "\n",
    "        if named_df_list:\n",
    "            for df in named_df_list:\n",
    "                if not save_dataframe_to_csv(df, str(df.name)):\n",
    "                    parsing_flag = False\n",
    "                    print(f'Failed to save DataFrame {df.name}.')\n",
    "                    break\n",
    "        else:\n",
    "            print('Error during reading the .pdf file occurred.')\n",
    "    else:\n",
    "        print('Parsing was skipped or done previously')\n",
    "\n",
    "\n",
    "\n",
    "def set_annotation(columns_list, df):\n",
    "    annotations = []\n",
    "\n",
    "    rev_column_list = columns_list[::-1]\n",
    "    for level, column in enumerate(rev_column_list):\n",
    "        num_unique_values = len(df[column].unique())\n",
    "        annotation = {\n",
    "            'text': f'<b>{column}</b>',\n",
    "            'x': 0.5,\n",
    "            'y': 1.05 - 0.17 * level,  # Adjust the vertical position\n",
    "            'xref': 'paper',\n",
    "            'yref': 'paper',\n",
    "            'showarrow': False,\n",
    "            'font': {'size': 10}\n",
    "        }\n",
    "        annotations.append(annotation)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "\n",
    "def build_tree_without_split(pdb_counts, df, protein_names_df, columns_list, df_name):\n",
    "    # Create a single sunburst diagram\n",
    "    top_proteins = pdb_counts.head(len(protein_names_df))\n",
    "    fig = px.sunburst(df, path=columns_list)\n",
    "\n",
    "    # Add annotations for levels of hierarchy\n",
    "    annotations = set_annotation(columns_list, df)\n",
    "    fig.update_layout(annotations=annotations)\n",
    "\n",
    "    output_path = f'../results/step1_pdf_parsing/plots/{df_name}_tree_diagram.svg'\n",
    "    plotly.io.write_image(fig, output_path, format='svg')\n",
    "\n",
    "\n",
    "\n",
    "def build_tree_with_split(protein_names_df, df, columns_list, df_name):\n",
    "    chunks = [protein_names_df[i:i + 4] for i in range(0, len(protein_names_df), 4)]\n",
    "\n",
    "    for i, chunk_proteins in enumerate(chunks, 1):\n",
    "        # Plot sunburst diagram for each chunk\n",
    "        chunk_df = df[df['Protein_Name'].isin(chunk_proteins['Protein_Name'])]\n",
    "        fig = px.sunburst(chunk_df, path=columns_list)\n",
    "\n",
    "        # Add annotations for levels of hierarchy\n",
    "        annotations = set_annotation(columns_list, df)\n",
    "        fig.update_layout(annotations=annotations)\n",
    "\n",
    "        output_path = f'../results/step1_pdf_parsing/plots/{df_name}_tree_diagram_part_{i}.svg'\n",
    "        plotly.io.write_image(fig, output_path, format='svg')\n",
    "\n",
    "\n",
    "\n",
    "def vz_tree_diagram(csv_paths, columns_list):\n",
    "    df_list = []\n",
    "\n",
    "    vz_flag = True\n",
    "\n",
    "    if vz_flag:\n",
    "        for csv in csv_paths:\n",
    "            df = pd.read_csv(csv)\n",
    "            df_list.append(df)\n",
    "            df_name = os.path.splitext(os.path.basename(csv))[0]\n",
    "\n",
    "            # Group by 'Protein_Name' and count the number of unique 'PDB_ID'\n",
    "            pdb_counts = df.groupby('Protein_Name')['PDB_ID'].nunique().reset_index()\n",
    "\n",
    "            # Sort the DataFrame by the number of PDB IDs in descending order\n",
    "            pdb_counts = pdb_counts.sort_values(by='PDB_ID', ascending=False)\n",
    "\n",
    "            # Create a new DataFrame with protein names in sorted order\n",
    "            protein_names_df = pdb_counts[['Protein_Name']]\n",
    "\n",
    "\n",
    "            if len(protein_names_df) < 50:\n",
    "                # Create a single sunburst diagram\n",
    "                build_tree_without_split(pdb_counts, df, protein_names_df, columns_list, df_name)\n",
    "\n",
    "            else:\n",
    "                # Split DataFrame into chunks of 4 protein_names and create sunburst diagram for each chunk\n",
    "                build_tree_with_split(protein_names_df, df, columns_list, df_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vz_pdb_id_distribution(csv_paths):\n",
    "\n",
    "    for csv in csv_paths:\n",
    "        df = pd.read_csv(csv)\n",
    "        df_name = os.path.splitext(os.path.basename(csv))[0]\n",
    "\n",
    "        # Group by 'Protein_Name' and count the number of unique 'PDB_ID'\n",
    "        pdb_counts = df.groupby('Protein_Name')['PDB_ID'].nunique().reset_index()\n",
    "\n",
    "        # Sort the DataFrame by the number of PDB IDs in descending order\n",
    "        pdb_counts = pdb_counts.sort_values(by='PDB_ID', ascending=False)\n",
    "\n",
    "        # Select the top 40 proteins\n",
    "        top_proteins = pdb_counts.head(40)\n",
    "\n",
    "        # Create a bar plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        ax = sns.barplot(x='Protein_Name', y='PDB_ID', data=top_proteins, palette='viridis')\n",
    "        plt.xlabel('Protein Name')\n",
    "        plt.ylabel('Number of Unique PDB IDs')\n",
    "        plt.title(f'Distribution of PDB IDs per Protein (Top 40) {df_name}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Set x-axis and y-axis ticks with step size 1\n",
    "        plt.xticks(range(len(top_proteins)), top_proteins['Protein_Name'])\n",
    "        plt.yticks(range(0, top_proteins['PDB_ID'].max() + 1, 10))\n",
    "\n",
    "        # Add annotations to each bar with exact number of PDB IDs\n",
    "        for i, v in enumerate(top_proteins['PDB_ID']):\n",
    "            ax.text(i, v + 0.1, str(v), ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "        output_path = f'../results/step1_pdf_parsing/plots/{df_name}_pdb_id_distribution.svg'\n",
    "        plt.savefig(output_path)\n",
    "\n",
    "\n",
    "\n",
    "def build_tree_for_common_elements(common_elements_dict, df_name, columns_list, df_list):\n",
    "    for df in df_list:\n",
    "        filtered_df = df[columns_list]\n",
    "\n",
    "        # Lists of values for each column\n",
    "        protein_name_list = common_elements_dict['Protein_Name']\n",
    "        pdb_id_list = common_elements_dict['PDB_ID']\n",
    "        chemical_id_list = common_elements_dict['Chemical_ID']\n",
    "\n",
    "        # Initialize a mask with True values\n",
    "        mask = pd.Series([True] * len(filtered_df), index=filtered_df.index)\n",
    "\n",
    "        # Apply filtering for each condition\n",
    "        for column, values in common_elements_dict.items():\n",
    "            mask &= filtered_df[column].isin(values)\n",
    "\n",
    "        # Filter the DataFrame based on the combined mask\n",
    "        plot_df = filtered_df[mask]\n",
    "\n",
    "        fig = px.sunburst(plot_df, path=columns_list)\n",
    "\n",
    "        # Add annotations for levels of hierarchy\n",
    "        annotations = set_annotation(columns_list, plot_df)\n",
    "        fig.update_layout(annotations=annotations)\n",
    "\n",
    "        output_path = f'../results/step1_pdf_parsing/plots/{df_name}_tree_diagram.svg'\n",
    "        plotly.io.write_image(fig, output_path, format='svg')\n",
    "        #print(plot_df)\n",
    "\n",
    "\n",
    "\n",
    "def vz_graph(df, columns_list, df_name, common_values):\n",
    "    # Assuming your filtered_df has columns: 'Protein_Name', 'PDB_ID', 'Chemical_ID'\n",
    "    G = nx.Graph()\n",
    "\n",
    "    # Add nodes and edges to the graph\n",
    "    for _, row in df.iterrows():\n",
    "        protein_name = row[columns_list[0]]\n",
    "        pdb_id = row[columns_list[1]]\n",
    "        chemical_id = row[columns_list[2]]\n",
    "\n",
    "        G.add_node(protein_name, level=1)  # Assign level 1 to protein_name\n",
    "        G.add_node(pdb_id, level=2)        # Assign level 2 to pdb_id\n",
    "        G.add_edge(protein_name, pdb_id)\n",
    "\n",
    "        G.add_node(chemical_id, level=3)   # Assign level 3 to chemical_id\n",
    "        G.add_edge(pdb_id, chemical_id)\n",
    "\n",
    "    # STEP 1\n",
    "    # Draw the graph with different colors for each level\n",
    "    levels = nx.get_node_attributes(G, 'level')\n",
    "    colors = [levels[node] for node in G.nodes]\n",
    "\n",
    "    # Create a list of node colors based on common_values\n",
    "    node_colors = ['red' if node in common_values else 'grey' for node in G.nodes]\n",
    "\n",
    "    # Create a list of edge colors based on the levels of the connected nodes\n",
    "    edge_colors = [levels[u] for u, v in G.edges]\n",
    "\n",
    "    plt.figure()\n",
    "    pos = nx.fruchterman_reingold_layout(G, scale=8.0, k=0.10, seed=131)\n",
    "\n",
    "    nx.draw(G, pos, with_labels=False, font_size=3, node_color=node_colors, cmap=plt.cm.spring,\n",
    "            node_size=10, font_color='black', font_weight='bold', width=1, edge_color=edge_colors)\n",
    "\n",
    "\n",
    "    # Draw node labels above the nodes for level 1 nodes\n",
    "    labels_level_1 = {node: node for node, level in levels.items() if level == 1}\n",
    "    node_label_color_level_1 = 'black'\n",
    "    nx.draw_networkx_labels(G, pos, labels_level_1, font_size=5, font_color=node_label_color_level_1,\n",
    "                            font_weight='bold', verticalalignment='bottom')\n",
    "\n",
    "    # Draw node labels above the nodes for level 2 nodes\n",
    "    labels_level_2 = {node: node for node, level in levels.items() if level == 2}\n",
    "    node_label_colors_level_2 = 'green'\n",
    "    nx.draw_networkx_labels(G, pos, labels_level_2, font_size=5, font_color=node_label_colors_level_2,\n",
    "                            font_weight='bold', verticalalignment='bottom')\n",
    "\n",
    "    # Draw node labels above the nodes for level 3 nodes\n",
    "    labels_level_3 = {node: node for node, level in levels.items() if level == 3}\n",
    "    node_label_colors_level_3 = 'purple'\n",
    "    nx.draw_networkx_labels(G, pos, labels_level_3, font_size=5, font_color= node_label_colors_level_3,\n",
    "                            font_weight='bold', verticalalignment='bottom')\n",
    "\n",
    "    # Save the graph as an SVG file\n",
    "    output_path = f'../results/step1_pdf_parsing/plots/{df_name}_common_vals_graph.svg'\n",
    "    plt.savefig(output_path, format='svg')\n",
    "\n",
    "\n",
    "\n",
    "    # STEP 2\n",
    "    # Identify connected components (subgraphs)\n",
    "    subgraphs = list(nx.connected_components(G))\n",
    "\n",
    "    # Define the layout for the main figure with graph and subplots\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    gs = gridspec.GridSpec(2, 2, width_ratios=[1, 1], height_ratios=[1, 0.5])\n",
    "\n",
    "    # Draw each connected component separately\n",
    "    for i, subgraph_nodes in enumerate(subgraphs):\n",
    "        subgraph = G.subgraph(subgraph_nodes)\n",
    "\n",
    "        levels = nx.get_node_attributes(subgraph, 'level')\n",
    "        colors = [levels[node] for node in subgraph.nodes]\n",
    "\n",
    "        # Create a list of node colors based on common_values\n",
    "        node_colors = ['red' if node in common_values else 'grey' for node in subgraph.nodes]\n",
    "\n",
    "        # Assign distinct colors to edges based on their levels\n",
    "        # edge_colors = [levels[u] for u, v in subgraph.edges]\n",
    "\n",
    "        # Define the position for the subplots\n",
    "        ax0 = plt.subplot(gs[0, 0])\n",
    "        ax1 = plt.subplot(gs[0, 1])\n",
    "        ax2 = plt.subplot(gs[1, :])\n",
    "\n",
    "        # Save each subgraph as an SVG file\n",
    "        output_path = f'../results/step1_pdf_parsing/plots/{df_name}_subgraph_{i + 1}.svg'\n",
    "\n",
    "        # Adjust the scale parameter to control the length of edges\n",
    "        pos = nx.fruchterman_reingold_layout(subgraph, scale=0.5, k=0.05, seed=131)\n",
    "\n",
    "        # Draw subgraph on the main figure\n",
    "        nx.draw(subgraph, pos, with_labels=False, font_size=14, node_color=node_colors, cmap=plt.cm.spring,\n",
    "                node_size=10, font_color='black', font_weight='bold', width=0.5, edge_color='black', ax=ax0)\n",
    "\n",
    "        # Draw node labels above the nodes for each level\n",
    "        for level_value in set(levels.values()):\n",
    "            labels_level = {node: node for node, level in levels.items() if level == level_value}\n",
    "            node_label_color_level = 'black' if level_value == 1 else 'green' if level_value == 2 else 'purple'\n",
    "            nx.draw_networkx_labels(G, pos, labels_level, font_size=8 if level_value == 1 else 5,\n",
    "                                    font_color=node_label_color_level, font_weight='bold', verticalalignment='bottom',\n",
    "                                    ax=ax0)\n",
    "\n",
    "        # Plot degree rank plot\n",
    "        degree_sequence = sorted((d for n, d in subgraph.degree()), reverse=True)\n",
    "        ax1.plot(degree_sequence, \"b-\", marker=\"o\")\n",
    "        ax1.set_title(\"Degree Rank Plot\")\n",
    "        ax1.set_ylabel(\"Degree\")\n",
    "        ax1.set_xlabel(\"Rank\")\n",
    "\n",
    "        # Plot degree histogram\n",
    "        ax2.bar(*np.unique(degree_sequence, return_counts=True))\n",
    "        ax2.set_title(\"Degree Histogram\")\n",
    "        ax2.set_xlabel(\"Degree\")\n",
    "        ax2.set_ylabel(\"# of Nodes\")\n",
    "\n",
    "        # Save the main figure with graph and subplots as an SVG file\n",
    "        plt.savefig(output_path, format='svg')\n",
    "\n",
    "        # Clear the figure for the next iteration\n",
    "        plt.clf()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vz_intersections(csv_paths, columns_list):\n",
    "    df_list = []\n",
    "    common_values = set()\n",
    "\n",
    "    # Check if any value in a row is in the common values list\n",
    "    def check_common_values(row):\n",
    "        return any(val in row.values for val in common_values)\n",
    "\n",
    "    for csv in csv_paths:\n",
    "        df = pd.read_csv(csv)\n",
    "        df = df[columns_list]\n",
    "        df_list.append(df)\n",
    "        df_name = os.path.splitext(os.path.basename(csv))[0]\n",
    "        #print(df_name)\n",
    "\n",
    "    common_vals_dict = csv_to_common_vals_dict(df_list, columns_list)\n",
    "    for values_list in common_vals_dict.values():\n",
    "        common_values.update(values_list)\n",
    "\n",
    "    #print(common_values)\n",
    "\n",
    "    for i, df in enumerate(df_list):\n",
    "        name_for_all_dataset = f'all_dataset_{i}'\n",
    "        # Apply the check_common_values function to filter rows\n",
    "        filtered_df = df[df.apply(check_common_values, axis=1)]\n",
    "        vz_graph(df, columns_list, name_for_all_dataset, common_values)\n",
    "        vz_graph(filtered_df, columns_list, str(i), common_values)\n",
    "\n",
    "        # Create a dictionary to map specific elements to colors\n",
    "        color_discrete_map = {}\n",
    "        for val in common_values:\n",
    "            color_discrete_map[val] = 'yellow'\n",
    "        #print(color_discrete_map)\n",
    "\n",
    "        fig = px.sunburst(filtered_df, path=columns_list, color_discrete_map=color_discrete_map)\n",
    "\n",
    "        # Add annotations for levels of hierarchy\n",
    "        annotations = set_annotation(columns_list, filtered_df)\n",
    "        fig.update_layout(annotations=annotations)\n",
    "\n",
    "        output_path = f'../results/step1_pdf_parsing/plots/{i}_tree_diagram_common.svg'\n",
    "        plotly.io.write_image(fig, output_path, format='svg')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def visualization(vz_flag, csv_directory):\n",
    "    if vz_flag:\n",
    "        csv_paths = glob.glob(os.path.join(csv_directory, '*.csv'))\n",
    "\n",
    "        # Plotting reports and writing DataFrames to CSV\n",
    "        columns_list = ['Protein_Name', 'PDB_ID', 'Chemical_ID']\n",
    "        #vz_comparison_of_csvs(csv_paths, columns_list)\n",
    "        #vz_pdb_id_distribution(csv_paths)\n",
    "        #vz_tree_diagram(csv_paths, columns_list)\n",
    "        vz_intersections(csv_paths, columns_list)\n",
    "\n",
    "    else:\n",
    "        print('Visualization was skipped of done previously.')\n",
    "\n",
    "\n",
    "def get_data_from_binding_db(uniprot_id, affinity_cutoff=None):\n",
    "    api_url = \"https://bindingdb.org/axis2/services/BDBService/getLigandsByUniprot\"\n",
    "\n",
    "    params = {\"uniprot\": uniprot_id}\n",
    "    if affinity_cutoff is not None:\n",
    "        params[\"IC50cutoff\"] = affinity_cutoff\n",
    "\n",
    "    params[\"response\"] = \"application/xml\"\n",
    "    response = requests.get(api_url, params=params)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.text\n",
    "    else:\n",
    "        print(f\"Error for UNIPROT ID {uniprot_id}: {response.status_code} - {response.text}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Get the directory of the current script and construct the relative paths\n",
    "    current_dir = os.path.dirname(__file__)\n",
    "    pdf_path = os.path.join(current_dir, '../data/data.pdf')\n",
    "    csv_directory = os.path.join(current_dir, '../results/step1_pdf_parsing/no_mapping_csvs')\n",
    "\n",
    "    # Flags for controlling\n",
    "    vz_flag = False\n",
    "    parsing_flag = False\n",
    "    mapping_flag = False\n",
    "    data_collection_flag = True\n",
    "\n",
    "    # Step 1. Parsing: pdf -> csv; cvs files creating at ../results/step1_pdf_parsing/no_mapping_csvs\n",
    "    parsing(parsing_flag, pdf_path)\n",
    "\n",
    "    # Step 2. Visualisation of csv: unique and common vals for a list of columns\n",
    "    visualization(vz_flag, csv_directory)\n",
    "\n",
    "    # Step 3. Mapping: PDB ID -> UniProt ID + BindingDB ID + Chembl ID\n",
    "    # The goal is to collect binding data from few resources\n",
    "    if mapping_flag:\n",
    "        csv_path = glob.glob(os.path.join(csv_directory, '*.csv'))\n",
    "        for csv in csv_path:\n",
    "            df_name = f'{os.path.splitext(os.path.basename(csv_path))[0]}'\n",
    "            df = pd.read_csv(csv_path)\n",
    "\n",
    "\n",
    "    # Step 4. Data collection for a chosen target CDK2 (Protein_Name) P24941 (Uniprot_ID)\n",
    "    #4.1. bindingDB request\n",
    "\n",
    "    if data_collection_flag:\n",
    "        uniprot_id = 'P24941'\n",
    "        ligand_data = get_data_from_binding_db(uniprot_id)\n",
    "\n",
    "        if ligand_data:\n",
    "            output_file_path = f\"../results/step2_data_collection/ligands_data_{uniprot_id}.xml\"\n",
    "            with open(output_file_path, \"w\") as xml_file:\n",
    "                xml_file.write(ligand_data)\n",
    "\n",
    "            print(f\"XML response written to {output_file_path}\")\n",
    "        else:\n",
    "            print(\"No data to write.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 4
}
